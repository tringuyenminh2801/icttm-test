{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will be used to contain data processing components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3 pyspark delta-spark python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the following to set the environment variables in the notebook if you don't set manually access key, secret key and endpoint in minio\n",
    "os.environ['OBJ_STORAGE_ACCESS_KEY'] = ''\n",
    "os.environ['OBJ_STORAGE_SECRET_KEY'] = ''\n",
    "os.environ['OBJ_STORAGE_ENDPOINT'] = 'http://localhost:9000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 storage\n",
    "obj_storage_access_key = os.getenv('OBJ_STORAGE_ACCESS_KEY')\n",
    "obj_storage_secret_key = os.getenv('OBJ_STORAGE_SECRET_KEY')\n",
    "obj_storage_endpoint = os.getenv('OBJ_STORAGE_ENDPOINT', 'http://localhost:9000')\n",
    "bucket_name = os.getenv('BUCKET_NAME', 'data')\n",
    "folder_name = os.getenv('FOLDER_NAME', 'data-raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1 = \"s3a://data/data-raw/data.json\"\n",
    "path_2 = \"s3a://data/data-raw/data2.json\"\n",
    "path_3 = \"s3a://data/data-raw/data3.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to more configuration if you want to use minio as object storage \n",
    "# (hint: maybe you can using .config() method to set the configuration if you want using spark to read/write data from/to minio)\n",
    "\n",
    "print(\"Configuring Spark...\")\n",
    "spark: SparkSession = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,io.delta:delta-core_2.12:2.1.0\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", obj_storage_endpoint) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", obj_storage_access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", obj_storage_secret_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"3\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", 'true') \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Done Spark configuration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the JSON file inside the 'data-raw' bucket, group them together and write into 'data-result' the grouped `Dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to read files from MinIO storage\n",
    "s3c = boto3.resource('s3',\n",
    "                    endpoint_url=obj_storage_endpoint,\n",
    "                    aws_access_key_id=obj_storage_access_key,\n",
    "                    aws_secret_access_key=obj_storage_secret_key,\n",
    "                    config=boto3.session.Config(signature_version='s3v4'),\n",
    "                    verify=False)\n",
    "bucket = s3c.Bucket(bucket_name)\n",
    "\n",
    "# Append all Dataframe read from bucket to this list\n",
    "df_list = []\n",
    "\n",
    "# List files inside the bucket\n",
    "# Read JSON file one by one and append it to a Dataframe list\n",
    "for object_summary in bucket.objects.filter(Prefix=f\"{folder_name}/\"):\n",
    "    current_df = spark.read.option(\"multiline\", \"true\").json(f\"s3a://{bucket_name}/{object_summary.key}\")\n",
    "    df_list.append(current_df)\n",
    "\n",
    "# Union them together to form a single Dataframe\n",
    "df: DataFrame = reduce(lambda l, r: l.unionByName(other=r), df_list)\n",
    "\n",
    "destination_folder = \"data-result\"\n",
    "destination_filename = \"result.json\"\n",
    "\n",
    "# Write to bucket\n",
    "df.write.json(f\"s3a://{bucket_name}/{destination_folder}/{destination_filename}\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicate the data inside `result.json` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the result.json file\n",
    "df = spark.read.json(path=f\"s3a://{bucket_name}/{destination_folder}/{destination_filename}\")\n",
    "df.show()\n",
    "print(f\"Number of rows before deduplication: {df.count()}\")\n",
    "assert df.count() == 19, \"Number of rows before deduplication should be 19\"\n",
    "# Deduplicate the dataframe by their id\n",
    "df = df.dropDuplicates(subset=['id'])\n",
    "print(f\"Number of rows after deduplication: {df.count()}\")\n",
    "assert df.count() == 18, \"Number of rows after deduplication should be 18\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the `Dataframe` so that it can be written and read in `CSV` file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "print(\"Write deduplicated Dataframe to MinIO storage\")\n",
    "df = df.selectExpr(\"id\", \"type\", \"name\", \"ppu\", \"batters.*\", \"topping\", \"filling\")\n",
    "\n",
    "array_cols = {\n",
    "    \"batter\" : [\"id\", \"type\"], \n",
    "    \"topping\" : [\"id\", \"type\"],\n",
    "    \"filling\" : [\"id\", \"name\"]\n",
    "}\n",
    "\n",
    "# Explode the array columns first, then select\n",
    "# all columns, with the array column renamed\n",
    "# to have its parent column name be the prefix\n",
    "df = df.withColumn(\"batter\", F.explode(\"batter\"))\\\n",
    "       .withColumn(\"topping\", F.explode(\"topping\"))\\\n",
    "       .withColumn(\"filling\", F.explode(\"filling\"))\\\n",
    "       .select([col for col in df.columns if col not in array_cols.keys()] \\\n",
    "               + [F.col(f\"{col}.{c}\").alias(f\"{col}_{c}\") \\\n",
    "                  for col in array_cols.keys() \\\n",
    "                  for c in df.withColumn(col, F.explode(col)).selectExpr(f\"{col}.*\").columns])\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "# We only want one CSV file, so repartition it to 1 and write to storage\n",
    "df.repartition(1).write.csv(f\"s3a://{bucket_name}/{destination_folder}/deduplicated-result.csv\", mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
